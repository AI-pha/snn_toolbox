% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\else\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}


\title{SNN toolbox Documentation}
\date{Aug 29, 2016}
\release{0.1}
\author{Bodo Rueckauer}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


This is a toolbox for converting analog to spiking neural networks (ANN to SNN),
and running them in a spiking neuron simulator.


\chapter{Citation}
\label{index:citation}\label{index:welcome-to-the-snn-toolbox-documentation}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{Diehl}\PYG{p}{,} \PYG{n}{P}\PYG{o}{.}\PYG{n}{U}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Neil}\PYG{p}{,} \PYG{n}{D}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Binas}\PYG{p}{,} \PYG{n}{J}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Cook}\PYG{p}{,} \PYG{n}{M}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Liu}\PYG{p}{,} \PYG{n}{S}\PYG{o}{.}\PYG{n}{C}\PYG{o}{.} \PYG{o+ow}{and} \PYG{n}{Pfeiffer}\PYG{p}{,} \PYG{n}{M}\PYG{o}{.}
\PYG{n}{Fast}\PYG{o}{\PYGZhy{}}\PYG{n}{Classifying}\PYG{p}{,} \PYG{n}{High}\PYG{o}{\PYGZhy{}}\PYG{n}{Accuracy} \PYG{n}{Spiking} \PYG{n}{Deep} \PYG{n}{Networks} \PYG{n}{Through} \PYG{n}{Weight} \PYG{o+ow}{and} \PYG{n}{Threshold} \PYG{n}{Balancing}\PYG{p}{,}
\PYG{n}{IEEE} \PYG{n}{International} \PYG{n}{Joint} \PYG{n}{Conference} \PYG{n}{on} \PYG{n}{Neural} \PYG{n}{Networks} \PYG{p}{(}\PYG{n}{IJCNN}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2015}
\end{Verbatim}
\begin{figure}[htbp]
\centering
\capstart

\scalebox{0.500000}{\includegraphics{{workflow}.png}}
\caption{\textbf{SNN toolbox workflow.} The input network (e.g. a Keras model) is parsed and all the necessary information
stored in an object that is independent of the input. This makes all following steps stable against changes
in the input, and allows straight-forward extension of the toolbox to include other input model libraries.
The parsed model can then be converted into a spiking network. The resulting SNN can be exported
for evaluation in a spiking simulator. At any stage of the pipeline, models and results can be written to disk.}\label{index:id1}\end{figure}


\chapter{Features}
\label{index:features}\begin{itemize}
\item {} 
Before conversion, the input model is parsed into a custom class containing only essential model structure and
weights in common python containers. This serves to abstract the core conversion process from possible input types.
The conversion toolbox currently supports input networks generated with Keras, Lasagne, or Caffe.
See {\hyperref[getting_started::doc]{\crossref{\DUrole{doc}{Getting Started}}}} on how to extend the relevant methods to handle models from other
common libraries like torch etc.

\item {} 
During parsing of the input model, several simplifications are performed to prepare the network for subsequent conversion:

\item {} 
During conversion of the analog neural network to spiking, the toolbox allows normalizing model parameters
for achieving higher accuracy in the converted net.

\item {} 
The resulting spiking network can then be exported to be tested in spiking simulators. The export format depends on the target
simulator. See {\hyperref[getting_started::doc]{\crossref{\DUrole{doc}{Getting Started}}}} on how to add a simulator to the toolbox.

\item {} 
The toolbox currently provides the following output formats:
\begin{itemize}
\item {} 
\href{http://neuralensemble.org/docs/PyNN/}{pyNN} models. pyNN is a simulator-independent language for building neural network
models. It allows running the converted net in a spiking simulator like \href{http://briansimulator.org/}{Brian},
\href{http://www.nest-simulator.org/}{Nest}, \href{https://www.neuron.yale.edu/neuron/}{Neuron},
or by a custom simulator that allows pyNN models as inputs.

\item {} 
Models to be run in \href{http://brian2.readthedocs.io/en/latest/index.html\#}{Brian2}.

\item {} 
An output format based on Keras models that can be run for instance on a built-in simulator developed at the University of Zurich.

\item {} 
The toolbox integrates MegaSim, an event-driven asynchronous spiking simulator developed at the University of Seville.

\end{itemize}

\item {} 
In addition to supporting the simulators listed above, the toolbox includes a ready-to-use
simulator developed at INI. This simulator features a very simple integrate-and-fire neuron.
By dispensing with redundant parameters and implementing a highly parallel simulation, the run time
is reduced by several orders of magnitude, without compromising accuracy.

\item {} 
Examples for both convolutional networks and fully-connected networks on MNIST and CIFAR10 are provided.

\item {} 
So far, this toolbox is able to handle classification datasets. For other applications,
the \code{io.load.get\_dataset} module needs to be extended.

\end{itemize}
\begin{figure}[htbp]
\centering
\capstart

\scalebox{1.000000}{\includegraphics{{gui_parameters}.png}}
\caption{\textbf{SNN toolbox GUI.} In the main window, the user can specify which tools to use during the experiment (e.g. whether or not to normalize weights prior to conversion, to evaluate the ANN before converting, to load an already converted net and simulate only, etc.). Also, parameters of the neuron cells used during simulation can be set. The GUI saves and reloads last settings automatically, and allows saving and loading preferences manually. Tooltips explain all functionality.}\label{index:id2}\end{figure}
\begin{figure}[htbp]
\centering
\capstart

\scalebox{0.500000}{\includegraphics{{gui_plots}.png}}
\caption{\textbf{SNN toolbox GUI plot window.} The toolbox looks for plots in the specified working directory. The user can select one or several layers, for which the results of a test run will be displayed in a way that facilitates examining and comparing results of each layer of the network. The example above compares ANN activations to SNN spikerates for the first convolutional layer on the MNIST dataset.}\label{index:id3}\end{figure}


\section{Getting Started}
\label{getting_started:getting-started}\label{getting_started::doc}

\subsection{Installation}
\label{getting_started:installation}
We recommend using virtual environments for different simulators, since the simulators
supported by pyNN may require different versions of python, numpy, ...
(Brian for instance only works with python-2).


\subsubsection{Requirements}
\label{getting_started:requirements}\begin{itemize}
\item {} 
All dependencies will be installed automatically.

\item {} 
For testing a converted network, the toolbox includes a ready-to-use spiking
simulator developed at INI. In addition, you may install a simulator supported
by \href{http://neuralensemble.org/docs/PyNN/}{pyNN}, or bring your own custom
simulator that accepts a pyNN or Keras model as input.

\end{itemize}


\subsubsection{Prebuild version}
\label{getting_started:prebuild-version}\begin{itemize}
\item {} 
Download the archive \code{dist/snntoolbox-\textless{}version\textgreater{}-py2.py3-none-any.whl}

\item {} 
Run \code{pip install snntoolbox-\textless{}version\textgreater{}-py2.py3-none-any.whl}.

\end{itemize}


\subsubsection{Development version}
\label{getting_started:development-version}\begin{itemize}
\item {} 
To get the latest version, checkout the repository

\item {} 
In the toolbox root directory \code{SNN\_toolbox/}, run \code{python setup.py develop}.

\end{itemize}


\subsection{Running the toolbox}
\label{getting_started:running-the-toolbox}
In a terminal window, type \code{snntoolbox} to start the main GUI containing all tools.

Alternitively, read and run example.py in snntoolbox/tests/, which contains a number of typical usecases.


\subsection{Extending the toolbox}
\label{getting_started:extending-the-toolbox}
Have a look at the \DUrole{xref,doc}{pipeline} module to examine the complete pipeline of
\begin{enumerate}
\item {} 
loading and testing a pretrained ANN,

\item {} 
normalizing weights

\item {} 
converting it to SNN,

\item {} 
running it on a simulator,

\item {} 
if given a specified hyperparameter range \code{params},
repeat simulations with modified parameters.

\end{enumerate}


\subsubsection{Input side: Adding a new model library}
\label{getting_started:input-side-adding-a-new-model-library}
So far, the toolbox supports input models written in Keras and Lasagne.

The philosophy behind the architecture is to make all steps in the conversion/simulation
pipeline independent of the original model format. Therefore, in order to add a
new input model library (e.g. Caffe) to the toolbox, put a module named \code{caffe\_input\_lib}
into the \code{model\_libs} package. Have a look at one of the existing files there to get an idea
what functions have to be implemented. The return requirements are specified in their
respective docstrings. Basically, all it needs is a function to parse the essential
information about layers into a common format used by the toolbox further down the line.


\subsubsection{Output side: Adding a custom simulator}
\label{getting_started:output-side-adding-a-custom-simulator}
Similarly, adding another simulator to run converted networks implies adding a file to the
\code{target\_simulators} package. Each file in there allow building a spiking network
and exporting it for use in a specific spiking simulator.

To add a simulator called `custom', put a file named \code{custom\_target\_sim.py} into \code{target\_simulators}. Then implement the class \code{SNN} with its
methods (\code{load}, \code{save}, \code{build}, \code{run}) tailored to `custom' simulator.


\subsection{Examples - Fully Connected Network on MNIST}
\label{getting_started:examples-fully-connected-network-on-mnist}
Normally, we would run the toolbox simply by typing \code{snntoolbox} in the terminal
and using the GUI.

If working with a python interpreter, one would specify a set of parameters and
then call \code{tests.util.test\_full()}, like this:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{snntoolbox}

\PYG{c+c1}{\PYGZsh{} Define parameters}
\PYG{n}{settings} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mnist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{architecture}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cnn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{99.16}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example/mnist/cnn/99.16/INI/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{evaluateANN}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Test accuracy of input model}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Normalize weights to get better perf.}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{convert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Convert analog net to spiking}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Simulate converted net}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Show plots and temporary results}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}thresh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Threshold potential}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulator}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{INI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Reset potential}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{100.0}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} Simulation time}

\PYG{c+c1}{\PYGZsh{} Run network (including loading the model, weight normalization, conversion}
\PYG{c+c1}{\PYGZsh{} and simulation)}
\PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{update\PYGZus{}setup}\PYG{p}{(}\PYG{n}{settings}\PYG{p}{)}
\PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{test\PYGZus{}full}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}

However, here are three usecases that allow some more insight into the application of this toolbox:
\begin{enumerate}
\item {} 
{\hyperref[getting_started:conversion\string-only]{\crossref{Conversion only}}}

\item {} 
{\hyperref[getting_started:simulation\string-only]{\crossref{Simulation only}}}

\item {} 
{\hyperref[getting_started:parameter\string-sweep]{\crossref{Parameter sweep}}}

\end{enumerate}

For a description of the possible values for the parameters in \code{settings},
see {\hyperref[configure_toolbox::doc]{\crossref{\DUrole{doc}{Configuration}}}}.
\phantomsection\label{getting_started:conversion-only}

\subsubsection{Usecase A - Conversion only}
\label{getting_started:usecase-a-conversion-only}\label{getting_started:conversion-only}\label{getting_started:spiking-network}\begin{description}
\item[{Steps:}] \leavevmode\begin{enumerate}
\item {} 
Set \code{convert = True} and \code{simulate = False}

\item {} 
Specify other parameters (working directory, filename, ...)

\item {} 
Update settings: \code{update\_setup(settings)}

\item {} 
Call \code{test\_full()}. This will
\begin{itemize}
\item {} 
load the dataset,

\item {} 
load a pretrained ANN from \code{\textless{}path\textgreater{}/\textless{}filename\textgreater{}}

\item {} 
optionally evaluate it (\code{evaluate = True}),

\item {} 
optionally normalize weights (\code{normalize = True}),

\item {} 
convert to spiking,

\item {} 
save SNN to disk.

\end{itemize}

\end{enumerate}

\end{description}
\phantomsection\label{getting_started:simulation-only}

\subsubsection{Usecase B - Simulation only}
\label{getting_started:evaluated}\label{getting_started:usecase-b-simulation-only}\label{getting_started:simulation-only}\begin{description}
\item[{Steps:}] \leavevmode\begin{enumerate}
\item {} 
Set \code{convert = False} and \code{simulate = True}

\item {} 
Specify other parameters (working directory, simulator to use, ...)

\item {} 
Update settings: \code{update\_setup(settings)}

\item {} 
Call \code{test\_full()}. This will
\begin{itemize}
\item {} 
load the dataset,

\item {} 
load your already converted SNN,

\item {} 
run the net on a spiking simulator,

\item {} 
plot spikerates, spiketrains, activations, correlations, etc.

\end{itemize}

\end{enumerate}

Note: It is assumed that a network has already been converted (e.g. with
Usecase A). I.e. there should be a folder in \code{\textless{}path\textgreater{}} containing the
converted network, named \code{snn\_\textless{}filename\textgreater{}\_\textless{}simulator\textgreater{}}.

\end{description}


\subsubsection{Usecase C - Parameter sweep}
\label{getting_started:parameter-sweep}\label{getting_started:usecase-c-parameter-sweep}\begin{description}
\item[{Steps:}] \leavevmode\begin{enumerate}
\item {} 
Specify parameters and update settings with \code{update\_setup(settings)}

\item {} 
Define a parameter range to sweep, e.g. for \titleref{v\_thresh}, using for
instance the helper function \code{get\_range()}

\item {} 
Call \code{test\_full}. This will
\begin{itemize}
\item {} 
load an already converted SNN or perform a conversion as specified in
settings.

\item {} 
run the SNN repeatedly on a spiking simulator while varying the
hyperparameter

\item {} 
plot accuracy vs. hyperparameter

\end{itemize}

\end{enumerate}

\end{description}

Usecase C is shown in full in the example below.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{snntoolbox}

\PYG{c+c1}{\PYGZsh{} Parameters}
\PYG{n}{settings} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mnist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{architecture}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cnn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{99.16}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{example/mnist/cnn/99.16/INI/}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{evaluateANN}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Test accuracy of input model}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Normalize weights to get better perf.}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{convert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Convert analog net to spiking}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{True}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Simulate converted net}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Show plots and temporary results}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}thresh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Threshold potential}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulator}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{INI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} Reset potential}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{100.0}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{\PYGZsh{} Simulation time}

\PYG{c+c1}{\PYGZsh{} Update defaults with parameters specified above:}
\PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{update\PYGZus{}setup}\PYG{p}{(}\PYG{n}{settings}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Run network (including loading the model, weight normalization,}
\PYG{c+c1}{\PYGZsh{} conversion and simulation).}

\PYG{c+c1}{\PYGZsh{} If set True, the converted model is simulated for three different values}
\PYG{c+c1}{\PYGZsh{} of v\PYGZus{}thresh. Otherwise use parameters as specified above,}
\PYG{c+c1}{\PYGZsh{} for a single run.}
\PYG{n}{do\PYGZus{}param\PYGZus{}sweep} \PYG{o}{=} \PYG{n+nb+bp}{True}
\PYG{k}{if} \PYG{n}{do\PYGZus{}param\PYGZus{}sweep}\PYG{p}{:}
    \PYG{n}{param\PYGZus{}name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}thresh}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{params} \PYG{o}{=} \PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{get\PYGZus{}range}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{test\PYGZus{}full}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n}{params}\PYG{p}{,}
                         \PYG{n}{param\PYGZus{}name}\PYG{o}{=}\PYG{n}{param\PYGZus{}name}\PYG{p}{,}
                         \PYG{n}{param\PYGZus{}logscale}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{snntoolbox}\PYG{o}{.}\PYG{n}{test\PYGZus{}full}\PYG{p}{(}\PYG{p}{)}
\end{Verbatim}


\subsection{Contact}
\label{getting_started:contact}\begin{itemize}
\item {} 
Bodo Rueckauer

\end{itemize}


\section{Configuration}
\label{configure_toolbox:module-snntoolbox.config}\label{configure_toolbox::doc}\label{configure_toolbox:configuration}\index{snntoolbox.config (module)}
Manage Parameters of SNNToolbox.
\begin{description}
\item[{In the GUI, the toolbox settings are grouped in three categories:}] \leavevmode\begin{enumerate}
\item {} 
Global parameters \code{globalparams}, specifying global settings for
loading / saving, and what steps of the workflow to include (evaluation,
normalization, conversion, simulation, ...)

\item {} 
Neuron cell parameters \code{cellparams}, determining properties of the
spiking neurons (e.g. threshold, refractory period, ...). Not all of
them are used in all simulators. For instance, our own simulator
\code{'INI'} only uses a threshold, reset and membrane time constant.

\item {} 
Simulation parameters \code{simparams}, specifying for instance length and
time resolution of the simulation run.

\end{enumerate}

\end{description}


\subsection{Parameters}
\label{configure_toolbox:parameters}

\subsubsection{Global Parameters}
\label{configure_toolbox:global-parameters}\begin{description}
\item[{dataset\_path: string}] \leavevmode
Where to load the dataset from. Used for testing the network. Dataset needs
to be in npy format.

\item[{model\_lib: string}] \leavevmode
The neural network library used to build the ANN, e.g.
\begin{itemize}
\item {} 
`keras'

\item {} 
`lasagne'

\item {} 
`caffe'

\end{itemize}

\item[{path\_wd: string, optional}] \leavevmode
Working directory. There, the toolbox will look for ANN models to convert
or SNN models to test, load the parameters it needs and store (normalized)
parameters.
If not specified, the toolbox will use as destination for all files it
needs to load and save:
\code{\textasciitilde{}/.snntoolbox/data/\textless{}filename\_ann\textgreater{}/\textless{}simulator\textgreater{}/}.
For instance, if we give \code{'98.29'} as filename of the ANN model to load,
and use default parameters otherwise, the toolbox will perform all
io-operations in \code{\textasciitilde{}/.snntoolbox/data/mnist/mlp/98.29/INI/}.

\item[{log\_dir\_of\_current\_run: string, optional}] \leavevmode
Path to directory where the output plots are stored. If not specified, will
be \code{\textless{}path\_wd\textgreater{}/log/gui/\textless{}runlabel\textgreater{}}. \code{\textless{}runlabel\textgreater{}} can be specified in the
GUI. Will be set to `test' if None.

\item[{filename\_ann: string}] \leavevmode
Base name of all loaded and saved files during this run. The ANN model to
be converted is expected to be named `\textless{}basename\textgreater{}'.

\item[{filename\_parsed\_model: string, optional}] \leavevmode
Name given to parsed SNN models. If not specified by the user, the
toolbox sets it to `\textless{}basename\textgreater{}\_parsed'.

\item[{filename\_snn: string, optional}] \leavevmode
Name given to converted spiking nets when exported to test it in a specific
simulator. If not specified by the user, the toolbox set it to
\code{snn\_\textless{}basename\textgreater{}\_\textless{}simulator\textgreater{}}.

\item[{evaluateANN: boolean, optional}] \leavevmode
If enabled, test the input model before and after it is parsed, to ensure
we do not lose performance. (Parsing extracts all necessary information
from the input model and creates a new network with some simplifications in
preparation for conversion to SNN.)
If you also enabled `normalization' (see parameter \code{normalize} below),
then the network will be evaluated again after normalization. This
operation should preserve accuracy as well.

\item[{normalize: boolean, optional}] \leavevmode
Only relevant when converting a network, not during simulation. If enabled,
the parameters of the spiking network will be normalized by the highest
activation value, or by the \code{n}-th percentile (see parameter
\code{percentile} below).

\item[{percentile: int, optional}] \leavevmode
Use the activation value in the specified percentile for normalization.
Set to \code{50} for the median, \code{100} for the max. Typical values are
\code{99, 99.9, 100}.

\item[{convert: boolean, optional}] \leavevmode
If enabled, load an ANN from \code{\textless{}path\_wd\textgreater{}} and convert it to spiking.

\item[{simulate: boolean, optional}] \leavevmode
If enabled, try to load SNN from \code{\textless{}path\_wd\textgreater{}} and test it on the specified
simulator (see parameter \code{simulator}).

\item[{overwrite: boolean, optional}] \leavevmode
If disabled, the save methods will ask for permission to overwrite files
before writing parameters, activations, models etc. to disk.

\item[{batch\_size: int, optional}] \leavevmode
If the builtin simulator `INI' is used, the batch size specifies
the number of test samples that will be simulated in parallel. Important:
When using `INI' simulator, the batch size can only be run usingthe batch
size it has been converted with. To run it with a different batch size,
convert the ANN from scratch.

\item[{verbose: int, optional}] \leavevmode
0: No intermediate results or status reports.
1: Print progress of simulation and intermediate results.
2: Record spiketrains of all layers for one sample, and save various plots
(spiketrains, spikerates, activations, correlations, ...)
3: Record, plot and return the membrane potential of all layers for the
last test sample. Very time consuming. Works only with pyNN simulators.

\item[{scaling\_factor: int, optional}] \leavevmode
Used by the MegaSim simulator to scale the neuron parameters and weights
because MegaSim uses integers.

\end{description}


\subsubsection{Cell Parameters}
\label{configure_toolbox:cell-parameters}\begin{description}
\item[{v\_thresh: float, optional}] \leavevmode
Threshold in mV defining the voltage at which a spike is fired.

\item[{v\_reset: float, optional}] \leavevmode
Reset potential in mV of the neurons after spiking.

\item[{v\_rest: float, optional}] \leavevmode
Resting membrane potential in mV.

\item[{e\_rev\_E: float, optional}] \leavevmode
Reversal potential for excitatory input in mV.

\item[{e\_rev\_I: float, optional}] \leavevmode
Reversal potential for inhibitory input in mV.

\item[{i\_offset: float, optional}] \leavevmode
Offset current in nA.

\item[{cm: float, optional}] \leavevmode
Membrane capacitance in nF.

\item[{tau\_m: float, optional}] \leavevmode
Membrane time constant in milliseconds.

\item[{tau\_refrac: float, optional}] \leavevmode
Duration of refractory period in milliseconds of the neurons after spiking.

\item[{tau\_syn\_E: float, optional}] \leavevmode
Decay time of the excitatory synaptic conductance in milliseconds.

\item[{tau\_syn\_I: float, optional}] \leavevmode
Decay time of the inhibitory synaptic conductance in milliseconds.

\item[{softmax\_clockrate: int, optional}] \leavevmode
In our implementation of a spiking softmax activation function we use an
external Poisson clock to trigger calculating the softmax of a layer. The
`softmax\_clockrate' parameter sets the firing rate in Hz of this external
clock. Note that this rate is limited by the maximum firing rate supported
by the simulator (given by the inverse time resolution 1000 * 1 / dt Hz).

\end{description}


\subsubsection{Simulation Parameters}
\label{configure_toolbox:simulation-parameters}\begin{description}
\item[{simulator: string, optional}] \leavevmode
Simulator with which to run the converted spiking network.

\item[{duration: float, optional}] \leavevmode
Runtime of simulation of one input in milliseconds.

\item[{dt: float, optional}] \leavevmode
Time resolution of spikes in milliseconds.

\item[{delay: float, optional}] \leavevmode
Delay in milliseconds. Must be equal to or greater than the resolution.

\item[{poisson\_input: float, optional}] \leavevmode
If enabled, the input samples will be converted to Poisson spiketrains. The
probability for a input neuron to fire is proportional to the analog value
of the corresponding pixel, and limited by the parameter `input\_rate'
below. For instance, with an `input\_rate' of 700, a fully-on pixel will
elicit a Poisson spiketrain of 700 Hz. Turn off for a less noisy
simulation. Currently, turning off Poisson input is only possible in INI
simulator.

\item[{reset: string, optional}] \leavevmode
Choose the reset mechanism to apply after spike.
Reset to zero: After spike, the membrane potential is set to the resting
potential.
Reset by subtraction: After spike, the membrane potential is reduced by a
value equal to the threshold.

\item[{input\_rate: float, optional}] \leavevmode
Poisson spike rate in Hz for a fully-on pixel of the input image. Note that
the input\_rate is limited by the maximum firing rate supported by the
simulator (given by the inverse time resolution 1000 * 1 / dt Hz).

\item[{normalization\_schedule: boolean, optional}] \leavevmode
Reduce the normalization factor each layer.

\item[{online\_normalization: boolean, optional}] \leavevmode
The converted spiking network performs best if the average firing rates of
each layer are not higher but also not much lower than the maximum rate
supported by the simulator (inverse time resolution). Normalization
eliminates saturation but introduces undersampling (parameters are
normalized with respect to the highest value in a batch). To overcome this,
the spikerates of each layer are monitored during simulation. If they drop
below the maximum firing rate by more than `diff to max rate', we set the
threshold of the layer to its highest rate.

\item[{diff\_to\_max\_rate: float, optional}] \leavevmode
If the highest firing rate of neurons in a layer drops below the maximum
firing rate by more than `diff to max rate', we set the threshold of the
layer to its highest rate. Set the parameter in Hz.

\item[{diff\_to\_min\_rate: float, optional}] \leavevmode
When The firing rates of a layer are below this value, the weights will NOT
be modified in the feedback mechanism described in `online\_normalization'.
This is useful in the beginning of a simulation, when higher layers need
some time to integrate up a sufficiently high membrane potential.

\item[{timestep\_fraction: int, optional}] \leavevmode
If set to 10 (default), the parameter modification mechanism described in
`online\_normalization' will be performed at every 10th timestep.

\item[{num\_to\_test: int, optional}] \leavevmode
How many samples to test.

\item[{maxpool\_type}] \leavevmode{[}string{]}
Implementation variants of spiking MaxPooling layers, based on
fir\_max: accmulated absolute firing rate
avg\_max: moving average of firing rate

\end{description}


\subsection{Default values}
\label{configure_toolbox:default-values}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{globalparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dataset\PYGZus{}path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}lib}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{keras}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{path\PYGZus{}wd}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log\PYGZus{}dir\PYGZus{}of\PYGZus{}current\PYGZus{}run}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename\PYGZus{}ann}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename\PYGZus{}parsed\PYGZus{}model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename\PYGZus{}snn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{snn\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{evaluateANN}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{percentile}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{99}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{convert}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{overwrite}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{verbose}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{\PYGZcb{}}
\PYG{n}{cellparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}thresh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}reset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{v\PYGZus{}rest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e\PYGZus{}rev\PYGZus{}E}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e\PYGZus{}rev\PYGZus{}I}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{i\PYGZus{}offset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.09}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tau\PYGZus{}m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1000}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tau\PYGZus{}refrac}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tau\PYGZus{}syn\PYGZus{}E}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tau\PYGZus{}syn\PYGZus{}I}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mf}{0.01}\PYG{p}{,}
              \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softmax\PYGZus{}clockrate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{300}\PYG{p}{\PYGZcb{}}
\PYG{n}{simparams} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{simulator}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{INI}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{duration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{200}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{delay}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{poisson\PYGZus{}input}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{k+kc}{False}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{reset}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Reset by subtraction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{input\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{1000}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{timestep\PYGZus{}fraction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{diff\PYGZus{}to\PYGZus{}max\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{200}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{num\PYGZus{}to\PYGZus{}test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{10}\PYG{p}{,}
             \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{diff\PYGZus{}to\PYGZus{}min\PYGZus{}rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{100}\PYG{p}{\PYGZcb{}}
\end{Verbatim}
\index{initialize\_simulator() (in module snntoolbox.config)}

\begin{fulllineitems}
\phantomsection\label{configure_toolbox:snntoolbox.config.initialize_simulator}\pysiglinewithargsret{\code{snntoolbox.config.}\bfcode{initialize\_simulator}}{\emph{simulator=None}}{}
Import module containing utility functions of spiking simulator.

\end{fulllineitems}

\index{update\_setup() (in module snntoolbox.config)}

\begin{fulllineitems}
\phantomsection\label{configure_toolbox:snntoolbox.config.update_setup}\pysiglinewithargsret{\code{snntoolbox.config.}\bfcode{update\_setup}}{\emph{s=None}}{}
Update parameters.

Check that parameter choices \code{s} are valid and update the global
parameter settings \code{snntoolbox.config.settings} with the user-specified
values. Default values are filled in where user did not give any.

\end{fulllineitems}



\section{Tools}
\label{modules:tools}\label{modules::doc}

\subsection{pipeline}
\label{core:pipeline}\label{core::doc}\label{core:module-core.pipeline}\index{core.pipeline (module)}
Wrapper script that combines all tools of SNN Toolbox.

Created on Thu May 19 16:37:29 2016

@author: rbodo
\index{is\_stop() (in module core.pipeline)}

\begin{fulllineitems}
\phantomsection\label{core:core.pipeline.is_stop}\pysiglinewithargsret{\code{core.pipeline.}\bfcode{is\_stop}}{\emph{queue}}{}
\end{fulllineitems}

\index{test\_full() (in module core.pipeline)}

\begin{fulllineitems}
\phantomsection\label{core:core.pipeline.test_full}\pysiglinewithargsret{\code{core.pipeline.}\bfcode{test\_full}}{\emph{queue=None, params={[}1{]}, param\_name='v\_thresh', param\_logscale=False}}{}
Convert an snn to a spiking neural network and simulate it.
\begin{description}
\item[{Complete pipeline of}] \leavevmode\begin{enumerate}
\item {} 
loading and testing a pretrained ANN,

\item {} 
normalizing parameters

\item {} 
converting it to SNN,

\item {} 
running it on a simulator,

\item {} 
if given a specified hyperparameter range \code{params},
repeat simulations with modified parameters.

\end{enumerate}

\item[{The testsuit allows specification of}] \leavevmode\begin{itemize}
\item {} 
the dataset (e.g. MNIST or CIFAR10)

\item {} 
the spiking simulator to use (currently Brian, Brian2, Nest, Neuron,
MegaSim or INI's simulator.)

\end{itemize}

\end{description}

Perform simulations of a spiking network, while optionally sweeping over a
specified hyper-parameter range. If the keyword arguments are not given,
the method performs a single run over the specified number of test samples,
using the updated default parameters.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{queue}} (\emph{\texttt{Queue, optional}}) -- Results are added to the queue to be displayed in the GUI.

\item {} 
\textbf{\texttt{params}} (\emph{\texttt{ndarray, optional}}) -- Contains the parameter values for which the simulation will be
repeated.

\item {} 
\textbf{\texttt{param\_name}} (\emph{\texttt{string, optional}}) -- Label indicating the parameter to sweep, e.g. \code{'v\_thresh'}.
Must be identical to the parameter's label in \code{globalparams}.

\item {} 
\textbf{\texttt{param\_logscale}} (\emph{\texttt{boolean, optional}}) -- If \code{True}, plot test accuracy vs \code{params} in log scale.
Defaults to \code{False}.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{results} -- List of the accuracies obtained after simulating with each parameter
value in param\_range.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/library/functions.html\#list}{list}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{inisim}
\label{core:module-core.inisim}\label{core:inisim}\index{core.inisim (module)}
INI spiking neuron simulator.

A collection of helper functions, including spiking layer classes derived from
Keras layers, which were used to implement our own IF spiking simulator.

Not needed when converting and running the SNN in other simulators (pyNN,
MegaSim, ...)

Created on Tue Dec  8 10:41:10 2015

@author: rbodo
\index{SpikeAveragePooling2D (class in core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeAveragePooling2D}\pysiglinewithargsret{\strong{class }\code{core.inisim.}\bfcode{SpikeAveragePooling2D}}{\emph{pool\_size=(2}, \emph{2)}, \emph{strides=None}, \emph{border\_mode='valid'}, \emph{label=None}, \emph{**kwargs}}{}
Bases: \code{keras.layers.pooling.AveragePooling2D}
\index{get\_name() (core.inisim.SpikeAveragePooling2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeAveragePooling2D.get_name}\pysiglinewithargsret{\bfcode{get\_name}}{}{}
Get class name.

\end{fulllineitems}

\index{get\_output() (core.inisim.SpikeAveragePooling2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeAveragePooling2D.get_output}\pysiglinewithargsret{\bfcode{get\_output}}{\emph{train=False}}{}
Get output.

\end{fulllineitems}


\end{fulllineitems}

\index{SpikeConvolution2D (class in core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeConvolution2D}\pysiglinewithargsret{\strong{class }\code{core.inisim.}\bfcode{SpikeConvolution2D}}{\emph{nb\_filter}, \emph{nb\_row}, \emph{nb\_col}, \emph{weights=None}, \emph{border\_mode='valid'}, \emph{subsample=(1}, \emph{1)}, \emph{label=None}, \emph{filter\_flip=True}, \emph{**kwargs}}{}
Bases: \code{keras.layers.convolutional.Convolution2D}

Spike 2D Convolution.
\index{get\_name() (core.inisim.SpikeConvolution2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeConvolution2D.get_name}\pysiglinewithargsret{\bfcode{get\_name}}{}{}
Get class name.

\end{fulllineitems}

\index{get\_output() (core.inisim.SpikeConvolution2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeConvolution2D.get_output}\pysiglinewithargsret{\bfcode{get\_output}}{\emph{train=False}}{}
Get output.

\end{fulllineitems}


\end{fulllineitems}

\index{SpikeDense (class in core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeDense}\pysiglinewithargsret{\strong{class }\code{core.inisim.}\bfcode{SpikeDense}}{\emph{output\_dim}, \emph{weights=None}, \emph{label=None}, \emph{**kwargs}}{}
Bases: \code{keras.layers.core.Dense}

Spike Dense layer.
\index{get\_name() (core.inisim.SpikeDense method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeDense.get_name}\pysiglinewithargsret{\bfcode{get\_name}}{}{}
Get class name.

\end{fulllineitems}

\index{get\_output() (core.inisim.SpikeDense method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeDense.get_output}\pysiglinewithargsret{\bfcode{get\_output}}{\emph{train=False}}{}
Get output.

\end{fulllineitems}


\end{fulllineitems}

\index{SpikeFlatten (class in core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeFlatten}\pysiglinewithargsret{\strong{class }\code{core.inisim.}\bfcode{SpikeFlatten}}{\emph{label=None}, \emph{**kwargs}}{}
Bases: \code{keras.layers.core.Flatten}

Spike flatten layer.
\index{get\_name() (core.inisim.SpikeFlatten method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeFlatten.get_name}\pysiglinewithargsret{\bfcode{get\_name}}{}{}
Get class name.

\end{fulllineitems}

\index{get\_output() (core.inisim.SpikeFlatten method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeFlatten.get_output}\pysiglinewithargsret{\bfcode{get\_output}}{\emph{train=False}}{}
Get output.

\end{fulllineitems}


\end{fulllineitems}

\index{SpikeMaxPooling2D (class in core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeMaxPooling2D}\pysiglinewithargsret{\strong{class }\code{core.inisim.}\bfcode{SpikeMaxPooling2D}}{\emph{pool\_size=(2}, \emph{2)}, \emph{strides=None}, \emph{border\_mode='valid'}, \emph{label=None}, \emph{pool\_type='fir\_max'}, \emph{**kwargs}}{}
Bases: \code{keras.layers.pooling.MaxPooling2D}

Max Pooling.
\index{get\_name() (core.inisim.SpikeMaxPooling2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeMaxPooling2D.get_name}\pysiglinewithargsret{\bfcode{get\_name}}{}{}
Get class name.

\end{fulllineitems}

\index{get\_output() (core.inisim.SpikeMaxPooling2D method)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.SpikeMaxPooling2D.get_output}\pysiglinewithargsret{\bfcode{get\_output}}{\emph{train=False}}{}
Get output.

\end{fulllineitems}


\end{fulllineitems}

\index{floatX() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.floatX}\pysiglinewithargsret{\code{core.inisim.}\bfcode{floatX}}{\emph{X}}{}
Return array in floatX settings of Theano.

\end{fulllineitems}

\index{get\_input() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.get_input}\pysiglinewithargsret{\code{core.inisim.}\bfcode{get\_input}}{\emph{self}}{}
Get input.

\end{fulllineitems}

\index{get\_new\_thresh() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.get_new_thresh}\pysiglinewithargsret{\code{core.inisim.}\bfcode{get\_new\_thresh}}{\emph{self}, \emph{time}}{}
Get new threshhold.

\end{fulllineitems}

\index{get\_time() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.get_time}\pysiglinewithargsret{\code{core.inisim.}\bfcode{get\_time}}{\emph{self}}{}
Get time.

\end{fulllineitems}

\index{get\_updates() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.get_updates}\pysiglinewithargsret{\code{core.inisim.}\bfcode{get\_updates}}{\emph{self}}{}
Get updates.

\end{fulllineitems}

\index{init\_layer() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.init_layer}\pysiglinewithargsret{\code{core.inisim.}\bfcode{init\_layer}}{\emph{self}, \emph{layer}, \emph{v\_thresh}, \emph{tau\_refrac}}{}
Init layer.

\end{fulllineitems}

\index{init\_neurons() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.init_neurons}\pysiglinewithargsret{\code{core.inisim.}\bfcode{init\_neurons}}{\emph{self}, \emph{v\_thresh=1.0}, \emph{tau\_refrac=0.0}, \emph{**kwargs}}{}
Init neurons.

\end{fulllineitems}

\index{linear\_activation() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.linear_activation}\pysiglinewithargsret{\code{core.inisim.}\bfcode{linear\_activation}}{\emph{self}, \emph{time}, \emph{updates}}{}
Linear activation.

\end{fulllineitems}

\index{on\_gpu() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.on_gpu}\pysiglinewithargsret{\code{core.inisim.}\bfcode{on\_gpu}}{}{}
Check if running on GPU board.

\end{fulllineitems}

\index{pool\_same\_size() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.pool_same_size}\pysiglinewithargsret{\code{core.inisim.}\bfcode{pool\_same\_size}}{\emph{data\_in}, \emph{patch\_size}, \emph{ignore\_border=True}, \emph{st=None}, \emph{padding=(0}, \emph{0)}}{}
Max-pooling in same size.

The indices of maximum values are 1s, else are 0s.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{data\_in}} (\emph{\texttt{4-D tensor.}}) -- input images. Max-pooling will be done over the 2 last dimensions.

\item {} 
\textbf{\texttt{patch\_size}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- with length 2 (patch height, patch width)

\item {} 
\textbf{\texttt{ignore\_border}} (\href{https://docs.python.org/library/functions.html\#bool}{\emph{\texttt{bool}}}) -- When True, (5,5) input with ds=(2,2) will generate a (2,2) output.
(3,3) otherwise.

\item {} 
\textbf{\texttt{st}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- Stride size, which is the number of shifts over rows/cols to get the
next pool region. If st is None, it is considered equal to ds
(no overlap on pooling regions).

\item {} 
\textbf{\texttt{padding}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- (pad\_h, pad\_w) pad zeros to extend beyond four borders of the
images, pad\_h is the size of the top and bottom margins, and
pad\_w is the size of the left and right margins.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.reset}\pysiglinewithargsret{\code{core.inisim.}\bfcode{reset}}{\emph{self}}{}
Reset.

\end{fulllineitems}

\index{sharedX() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.sharedX}\pysiglinewithargsret{\code{core.inisim.}\bfcode{sharedX}}{\emph{X}, \emph{dtype='float32'}, \emph{name=None}}{}
Make array as shared array.

\end{fulllineitems}

\index{shared\_zeros() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.shared_zeros}\pysiglinewithargsret{\code{core.inisim.}\bfcode{shared\_zeros}}{\emph{shape}, \emph{dtype='float32'}, \emph{name=None}}{}
Make shared zeros array.

\end{fulllineitems}

\index{skip\_spike() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.skip_spike}\pysiglinewithargsret{\code{core.inisim.}\bfcode{skip\_spike}}{\emph{new\_mem}}{}
Skip spike.

\end{fulllineitems}

\index{softmax\_activation() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.softmax_activation}\pysiglinewithargsret{\code{core.inisim.}\bfcode{softmax\_activation}}{\emph{self}, \emph{time}, \emph{updates}}{}
Softmax activation.

\end{fulllineitems}

\index{trigger\_spike() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.trigger_spike}\pysiglinewithargsret{\code{core.inisim.}\bfcode{trigger\_spike}}{\emph{new\_mem}}{}
Trigger spike.

\end{fulllineitems}

\index{update\_neurons() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.update_neurons}\pysiglinewithargsret{\code{core.inisim.}\bfcode{update\_neurons}}{\emph{self}, \emph{time}, \emph{updates}}{}
Update neurons according to activation function.

\end{fulllineitems}

\index{update\_payload() (in module core.inisim)}

\begin{fulllineitems}
\phantomsection\label{core:core.inisim.update_payload}\pysiglinewithargsret{\code{core.inisim.}\bfcode{update\_payload}}{\emph{self}, \emph{new\_mem}, \emph{spikes}, \emph{time}}{}
Update payloads.

\end{fulllineitems}



\subsection{megasim}
\label{core:module-core.megasim}\label{core:megasim}\index{core.megasim (module)}
MegaSim spiking neuron simulator

A collection of helper functions used to get MegaSim's path and executable.

the configuration file will be stored at \$HOME/.snntoolbox/preferences/megasim\_config.json

Assumes that have write access to the home folder.

Created on Tue Dec  8 10:41:10 2015

@author: evan
\index{megasim\_path() (in module core.megasim)}

\begin{fulllineitems}
\phantomsection\label{core:core.megasim.megasim_path}\pysiglinewithargsret{\code{core.megasim.}\bfcode{megasim\_path}}{}{}
\end{fulllineitems}



\subsection{util}
\label{core:util}\label{core:module-core.util}\index{core.util (module)}
Helper functions to handle parameters and variables of interest during
conversion and simulation of an SNN.

Created on Wed Mar  9 16:18:33 2016

@author: rbodo
\index{extract\_label() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.extract_label}\pysiglinewithargsret{\code{core.util.}\bfcode{extract\_label}}{\emph{label}}{}
Get the layer number, name and shape from a string.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{label}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Specifies both the layer type, index and shape, e.g.
\code{'03Convolution2D\_3x32x32'}.

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\textbf{layer\_num} (\emph{int}) -- The index of the layer in the network.

\item {} 
\textbf{name} (\emph{string}) -- The type of the layer.

\item {} 
\textbf{shape} (\emph{tuple}) -- The shape of the layer

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{get\_activ\_fn\_for\_layer() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_activ_fn_for_layer}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_activ\_fn\_for\_layer}}{\emph{model}, \emph{i}}{}
\end{fulllineitems}

\index{get\_activations\_batch() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_activations_batch}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_activations\_batch}}{\emph{ann}, \emph{X\_batch}}{}
Compute layer activations of an ANN.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{ann}} (\emph{\texttt{Keras model}}) -- Needed to compute activations.

\item {} 
\textbf{\texttt{X\_batch}} (\emph{\texttt{float32 array}}) -- The input samples to use for determining the layer activations. With
data of the form (channels, num\_rows, num\_cols), X has dimension
(batch\_size, channels*num\_rows*num\_cols) for a multi-layer perceptron,
and (batch\_size, channels, num\_rows, num\_cols) for a convolutional net.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{activations\_batch} -- Each entry represents a layer in the ANN for which an activation can be
calculated (e.g. \code{Dense}, \code{Convolution2D}).
\code{activations} containing the activations of a layer. It has the same
shape as the original layer, e.g.
(batch\_size, n\_features, n\_rows, n\_cols) for a convolution layer.
\code{label} is a string specifying the layer type, e.g. \code{'Dense'}.

\item[{Return type}] \leavevmode
list of tuples \code{(activations, label)}

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_activations\_layer() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_activations_layer}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_activations\_layer}}{\emph{get\_activ}, \emph{X\_train}}{}
Get activations of a specific layer, iterating batch-wise over the complete
data set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{get\_activ}} (\emph{\texttt{Theano function}}) -- A Theano function computing the activations of a layer.

\item {} 
\textbf{\texttt{X\_train}} (\emph{\texttt{float32 array}}) -- The samples to compute activations for. With data of the form
(channels, num\_rows, num\_cols), X\_train has dimension
(batch\_size, channels*num\_rows*num\_cols) for a multi-layer perceptron,
and (batch\_size, channels, num\_rows, num\_cols) for a convolutional net.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{activations} -- The activations of cells in a specific layer. Has the same shape as the
layer.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/library/array.html\#module-array}{array}

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_range() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_range}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_range}}{\emph{start=0.0}, \emph{stop=1.0}, \emph{num=5}, \emph{method='linear'}}{}
Return a range of parameter values.

Convenience function. For more flexibility, use \code{numpy.linspace},
\code{numpy.logspace}, \code{numpy.random.random\_sample} directly.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{start}} (\emph{\texttt{scalar, optional}}) -- The starting value of the sequence

\item {} 
\textbf{\texttt{stop}} (\emph{\texttt{scalar, optional}}) -- End value of the sequence.

\item {} 
\textbf{\texttt{num}} (\emph{\texttt{int, optional}}) -- Number of samples to generate. Must be non-negative.

\item {} 
\textbf{\texttt{method}} (\emph{\texttt{string, optional}}) -- The sequence will be computed on either a linear, logarithmic or random
grid.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{samples} -- There are \code{num} samples in the closed interval {[}start, stop{]}.

\item[{Return type}] \leavevmode
ndarray

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_sample\_activity\_from\_batch() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_sample_activity_from_batch}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_sample\_activity\_from\_batch}}{\emph{activity\_batch}, \emph{idx=0}}{}
Return layer activity for sample \code{idx} of an \code{activity\_batch}.

\end{fulllineitems}

\index{get\_scale\_fac() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.get_scale_fac}\pysiglinewithargsret{\code{core.util.}\bfcode{get\_scale\_fac}}{\emph{activations}, \emph{idx=0}}{}
Determine the maximum activation of a layer.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{activations}} (\href{https://docs.python.org/library/array.html\#module-array}{\emph{\texttt{array}}}) -- The activations of cells in a specific layer, flattened to 1-d.

\item {} 
\textbf{\texttt{idx}} (\emph{\texttt{int, optional}}) -- The index of the layer. May be used to decrease the scale factor in
higher layers, to maintain high spike rates.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{scale\_fac} -- Maximum (or percentile) of activations in this layer.
Parameters of the respective layer are scaled by this value.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/library/functions.html\#float}{float}

\end{description}\end{quote}

\end{fulllineitems}

\index{normalize\_parameters() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.normalize_parameters}\pysiglinewithargsret{\code{core.util.}\bfcode{normalize\_parameters}}{\emph{model}}{}
Normalize the parameters of a network.

The parameters of each layer are normalized with respect to the maximum
activation, or the \code{n}-th percentile of activations.

Generates plots of the activity- and weight-distribution before and after
normalization. Note that plotting the activity-distribution can be very
time- and memory-consuming for larger networks.

\end{fulllineitems}

\index{parse() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.parse}\pysiglinewithargsret{\code{core.util.}\bfcode{parse}}{\emph{input\_model}}{}
Create a Keras model suitable for conversion to SNN.

This parsing function takes as input an arbitrary neural network and builds
a Keras model from it with the same functionality and performance.
The resulting model contains all essential information about the network,
independently of the model library in which the original network was built
(e.g. Caffe). This makes the SNN toolbox stable against changes in input
formats. Another advantage is extensibility: In order to add a new input
language to the toolbox (e.g. Lasagne), a developer only needs to add a
single module to \code{model\_libs} package, implementing a number of methods
(see the respective functions in `keras\_input\_lib.py' for more details.)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{input\_model}} (\emph{\texttt{Analog Neural Network}}) -- A pretrained neural network model.

\item[{Returns}] \leavevmode
\textbf{parsed\_model} -- A Keras model functionally equivalent to \code{input\_model}.

\item[{Return type}] \leavevmode
Keras model

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_description() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.print_description}\pysiglinewithargsret{\code{core.util.}\bfcode{print\_description}}{\emph{snn=None}, \emph{log=True}}{}
Print a summary of the test run, parameters, and network. If \code{log==True},
the output is written as \code{settings.txt} file to the folder given by
\code{settings{[}'log\_dir\_of\_current\_run'{]}}.

\end{fulllineitems}

\index{spiketrains\_to\_rates() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.spiketrains_to_rates}\pysiglinewithargsret{\code{core.util.}\bfcode{spiketrains\_to\_rates}}{\emph{spiketrains\_batch}}{}
Convert spiketrains to spikerates.

The output will have the same shape as the input except for the last
dimension, which is removed by replacing a sequence of spiketimes by a
single rate value.

\end{fulllineitems}

\index{wilson\_score() (in module core.util)}

\begin{fulllineitems}
\phantomsection\label{core:core.util.wilson_score}\pysiglinewithargsret{\code{core.util.}\bfcode{wilson\_score}}{\emph{p}, \emph{n}}{}
Confidence interval of a binomial distribution.

See \url{https://en.wikipedia.org/wiki/Binomial\_proportion\_confidence\_interval}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{p}} (\href{https://docs.python.org/library/functions.html\#float}{\emph{\texttt{float}}}) -- The proportion of successes in \code{n} experiments.

\item {} 
\textbf{\texttt{n}} (\href{https://docs.python.org/library/functions.html\#int}{\emph{\texttt{int}}}) -- The number of Bernoulli-trials (sample size).

\end{itemize}

\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
The confidence interval.

\end{description}\end{quote}

\end{fulllineitems}


\code{source}


\subsection{cifar10}
\label{snntoolbox.io_utils:module-snntoolbox.io_utils.cifar10_load}\label{snntoolbox.io_utils:cifar10}\label{snntoolbox.io_utils::doc}\index{snntoolbox.io\_utils.cifar10\_load (module)}
Helper functions to load, process, augment and save Cifar10 dataset.

Created on Mon Jun  6 12:55:10 2016

@author: rbodo
\index{get\_cifar10() (in module snntoolbox.io\_utils.cifar10\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.cifar10_load.get_cifar10}\pysiglinewithargsret{\code{snntoolbox.io\_utils.cifar10\_load.}\bfcode{get\_cifar10}}{\emph{path=None}, \emph{filename=None}, \emph{flat=False}}{}
Get cifar10 classification dataset.

Values are normalized and saved as \code{float32} type. Class vectors are
converted to binary class matrices. Output can be flattened for use in
fully-connected networks. Can perform preprocessing using a Keras
ImageDataGenerator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the loaded and modified dataset is saved to
\code{path} directory.

\item {} 
\textbf{\texttt{filename}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the dataset will be written to \code{filename}.
If \code{filename} is not specified, use \code{cifar10} or \code{cifar10\_flat}.

\item {} 
\textbf{\texttt{flat}} (\emph{\texttt{Boolean, optional}}) -- If \code{True}, the output is flattened. Defaults to \code{False}.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
Three compressed files \code{path/filename\_X\_norm.npz},

\item {} 
\code{path/filename\_X\_test.npz}, and \code{path/filename\_Y\_test.npz}.

\item {} 
With data of the form (channels, num\_rows, num\_cols), \code{X\_norm} and

\item {} 
\code{X\_test} have dimension (num\_samples, channels*num\_rows*num\_cols)

\item {} 
in case \code{flat==True}, and (num\_samples, channels, num\_rows, num\_cols)

\item {} 
otherwise. \code{Y\_test} has dimension (num\_samples, num\_classes).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}



\subsection{caltech101}
\label{snntoolbox.io_utils:caltech101}\label{snntoolbox.io_utils:module-snntoolbox.io_utils.caltech101_load}\index{snntoolbox.io\_utils.caltech101\_load (module)}
Helper functions to load, process, augment and save Caltech101 dataset.

Created on Mon Jun  6 12:55:20 2016

@author: rbodo
\index{get\_caltech101() (in module snntoolbox.io\_utils.caltech101\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.caltech101_load.get_caltech101}\pysiglinewithargsret{\code{snntoolbox.io\_utils.caltech101\_load.}\bfcode{get\_caltech101}}{\emph{path}, \emph{filename=None}}{}
Get caltech101 classification dataset.

Values are normalized and saved as \code{float32} type. Class vectors are
converted to binary class matrices. Output can be flattened for use in
fully-connected networks. Can perform preprocessing using a Keras
ImageDataGenerator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the loaded and modified dataset is saved to
\code{path} directory.

\item {} 
\textbf{\texttt{filename}} (\emph{\texttt{string, optional}}) -- Basename of file to create. Individual files will be appended
\code{\_X\_norm}, \code{\_X\_test}, etc.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
Three compressed files \code{path/filename\_X\_norm.npz},

\item {} 
\code{path/filename\_X\_test.npz}, and \code{path/filename\_Y\_test.npz}.

\item {} 
With data of the form (channels, num\_rows, num\_cols), \code{X\_norm} and

\item {} 
\code{X\_test} have dimension (num\_samples, channels, num\_rows, num\_cols).

\item {} 
\code{Y\_test} has dimension (num\_samples, num\_classes).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}



\subsection{mnist}
\label{snntoolbox.io_utils:mnist}\label{snntoolbox.io_utils:module-snntoolbox.io_utils.mnist_load}\index{snntoolbox.io\_utils.mnist\_load (module)}
Helper functions to load, process, augment and save MNIST dataset.

Created on Mon Jun  6 12:54:49 2016

@author: rbodo
\index{get\_mnist() (in module snntoolbox.io\_utils.mnist\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.mnist_load.get_mnist}\pysiglinewithargsret{\code{snntoolbox.io\_utils.mnist\_load.}\bfcode{get\_mnist}}{\emph{path=None}, \emph{filename=None}, \emph{flat=False}}{}
Get mnist classification dataset.

Values are normalized and saved as \code{float32} type. Class vectors are
converted to binary class matrices. Output can be flattened for use in
fully-connected networks.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the loaded and modified dataset is saved to
\code{path} directory.

\item {} 
\textbf{\texttt{filename}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the dataset will be written to \code{filename}.
If \code{filename} is not specified, use \code{mnist} or \code{mnist\_flat}.

\item {} 
\textbf{\texttt{flat}} (\emph{\texttt{Boolean, optional}}) -- If \code{True}, the output is flattened. Defaults to \code{False}.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{The dataset as a tuple containing the training and test sample arrays}

\item {} 
\emph{(X\_train, Y\_train, X\_test, Y\_test).}

\item {} 
With data of the form (channels, num\_rows, num\_cols), \code{X\_train} and

\item {} 
\code{X\_test} have dimension (num\_samples, channels*num\_rows*num\_cols)

\item {} 
in case \code{flat==True}, and

\item {} 
\emph{(num\_samples, channels, num\_rows, num\_cols) otherwise.}

\item {} 
\code{Y\_train} and \code{Y\_test} have dimension (num\_samples, num\_classes).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}



\subsection{facedetection}
\label{snntoolbox.io_utils:module-snntoolbox.io_utils.facedetection_load}\label{snntoolbox.io_utils:facedetection}\index{snntoolbox.io\_utils.facedetection\_load (module)}
Helper functions to load, process, augment and save facedetection dataset.

Created on Mon Jun  6 12:55:20 2016

@author: rbodo
\index{get\_facedetection() (in module snntoolbox.io\_utils.facedetection\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.facedetection_load.get_facedetection}\pysiglinewithargsret{\code{snntoolbox.io\_utils.facedetection\_load.}\bfcode{get\_facedetection}}{\emph{sourcepath}, \emph{imagepath}, \emph{targetpath=None}, \emph{filename=None}}{}
Get facedetection dataset.

Values are normalized and saved as \code{float32} type. Class vectors are
converted to binary class matrices. Output can be flattened for use in
fully-connected networks.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{sourcepath}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Where to find text file containing the filenames and labels of the
image samples.

\item {} 
\textbf{\texttt{imagepath}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Path to image folder.

\item {} 
\textbf{\texttt{targetpath}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the loaded and modified dataset is saved to
\code{path} directory.

\item {} 
\textbf{\texttt{filename}} (\emph{\texttt{string, optional}}) -- If a \code{path} is given, the dataset will be written to \code{filename}.
If \code{filename} is not specified, use \code{facedetection} or
\code{facedetection\_flat}.

\end{itemize}

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{The dataset as a tuple containing the training and test sample arrays}

\item {} 
\emph{(X\_train, Y\_train, X\_test, Y\_test).}

\item {} 
With data of the form (channels, num\_rows, num\_cols), \code{X\_train} and

\item {} 
\code{X\_test} have dimension (num\_samples, channels, num\_rows, num\_cols).

\item {} 
\code{Y\_train} and \code{Y\_test} have dimension (num\_samples, num\_classes).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{load\_paths\_from\_files() (in module snntoolbox.io\_utils.facedetection\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.facedetection_load.load_paths_from_files}\pysiglinewithargsret{\code{snntoolbox.io\_utils.facedetection\_load.}\bfcode{load\_paths\_from\_files}}{\emph{sourcepath}, \emph{imagepath}, \emph{filename}}{}
\end{fulllineitems}

\index{load\_samples() (in module snntoolbox.io\_utils.facedetection\_load)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.facedetection_load.load_samples}\pysiglinewithargsret{\code{snntoolbox.io\_utils.facedetection\_load.}\bfcode{load\_samples}}{\emph{filepaths}, \emph{nb\_samples=None}}{}
\end{fulllineitems}



\subsection{plotting}
\label{snntoolbox.io_utils:plotting}\label{snntoolbox.io_utils:module-snntoolbox.io_utils.plotting}\index{snntoolbox.io\_utils.plotting (module)}
Various functions to visualize connectivity, activity and accuracy of the
network.

Created on Wed Nov 18 13:57:37 2015

@author: rbodo
\index{output\_graphs() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.output_graphs}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{output\_graphs}}{\emph{spiketrains\_batch}, \emph{activations\_batch}, \emph{path=None}, \emph{idx=0}}{}
Wrapper function to display / save a number of plots.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{spiketrains\_batch}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- Each entry in \code{spiketrains\_batch} contains a tuple
\code{(spiketimes, label)} for each layer of the network (for the first
batch only, and excluding \code{Flatten} layers).
\code{spiketimes} is an array where the last index contains the spike
times of the specific neuron, and the first indices run over the
number of neurons in the layer:
(batch\_size, n\_chnls, n\_rows, n\_cols, duration)
\code{label} is a string specifying both the layer type and the index,
e.g. \code{'03Dense'}.

\item {} 
\textbf{\texttt{activations\_batch}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- Activations of the SNN.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\item {} 
\textbf{\texttt{idx}} (\emph{\texttt{int, optional}}) -- The index of the sample to display. Defaults to 0.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_confusion\_matrix() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_confusion_matrix}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_confusion\_matrix}}{\emph{Y\_test}, \emph{Y\_pred}, \emph{path=None}, \emph{class\_labels=None}}{}
\end{fulllineitems}

\index{plot\_correlations() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_correlations}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_correlations}}{\emph{spikerates}, \emph{layer\_activations}}{}
Plot the correlation between SNN spiketrains and ANN activations.

For each layer, the method draws a scatter plot, showing the correlation
between the average firing rate of neurons in the SNN layer and the
activation of the corresponding neurons in the ANN layer.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{spikerates}} (list of tuples \code{(spikerate, label)}.) -- 
\code{spikerate} is a 1D array containing the mean firing rates of the
neurons in a specific layer.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'3Dense'}.


\item {} 
\textbf{\texttt{layer\_activations}} (list of tuples \code{(activations, label)}) -- 
Each entry represents a layer in the ANN for which an activation can be
calculated (e.g. \code{Dense}, \code{Convolution2D}).

\code{activations} is an array of the same dimension as the corresponding
layer, containing the activations of Dense or Convolution layers.

\code{label} is a string specifying the layer type, e.g. \code{'Dense'}.


\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_error\_vs\_time() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_error_vs_time}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_error\_vs\_time}}{\emph{err}, \emph{path=None}}{}
\end{fulllineitems}

\index{plot\_hist() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_hist}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_hist}}{\emph{h}, \emph{title=None}, \emph{layer\_label=None}, \emph{path=None}, \emph{scale\_fac=None}}{}
Plot a histogram over two datasets.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{h}} (\href{https://docs.python.org/library/stdtypes.html\#dict}{\emph{\texttt{dict}}}) -- Dictionary of datasets to plot in histogram.

\item {} 
\textbf{\texttt{title}} (\emph{\texttt{string, optional}}) -- Title of histogram.

\item {} 
\textbf{\texttt{layer\_label}} (\emph{\texttt{string, optional}}) -- Label of layer from which data was taken.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\item {} 
\textbf{\texttt{scale\_fac}} (\emph{\texttt{float, optional}}) -- The value with which parameters are normalized (maximum of activations
or parameter value of a layer). If given, will be insterted into plot
title.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_hist\_combined() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_hist_combined}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_hist\_combined}}{\emph{data}, \emph{path=None}}{}
Plot a histogram over several datasets.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{data}} (\href{https://docs.python.org/library/stdtypes.html\#dict}{\emph{\texttt{dict}}}) -- Dictionary of datasets to plot in histogram.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_history() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_history}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_history}}{\emph{h}}{}
Plot the training and validation loss and accuracy at each epoch.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{h}} (\emph{\texttt{Keras history object}}) -- Contains the training and validation loss and accuracy at each epoch
during training.

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_layer\_activity() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_layer_activity}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_layer\_activity}}{\emph{layer}, \emph{title}, \emph{path=None}, \emph{limits=None}}{}
Visualize a layer by arranging the neurons in a line or on a 2D grid.

Can be used to show average firing rates of individual neurons in an SNN,
or the activation function per layer in an ANN.
The activity is encoded by color.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{layer}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- 
\code{(activity, label)}.

\code{activity} is an array of the same shape as the original layer,
containing e.g. the spikerates or activations of neurons in a layer.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'3Dense'}.


\item {} 
\textbf{\texttt{title}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Figure title.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\item {} 
\textbf{\texttt{limits}} (\emph{\texttt{tuple, optional}}) -- If not \code{None}, the colormap of the resulting image is limited by this
tuple.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_layer\_correlation() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_layer_correlation}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_layer\_correlation}}{\emph{rates}, \emph{activations}, \emph{title}, \emph{path=None}}{}
Plot correlation between spikerates and activations of a specific layer,
as 2D-dot-plot.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{rates}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- The spikerates of a layer, flattened to 1D.

\item {} 
\textbf{\texttt{activations}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- The activations of a layer, flattened to 1D.

\item {} 
\textbf{\texttt{title}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Plot title.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_layer\_summaries() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_layer_summaries}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_layer\_summaries}}{\emph{spikerates}, \emph{activations}, \emph{spiketrains=None}, \emph{path=None}}{}
Display or save a number of plots for a specific layer.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{spikerates}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- 
Each entry in \code{spikerates} contains a tuple \code{(rates, label)} for
each layer of the network (for the first batch only, and excluding
\code{Flatten} layers).

\code{rates} contains the average firing rates of all neurons in a layer.
It has the same shape as the original layer, e.g.
(n\_features, n\_rows, n\_cols) for a convolution layer.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'03Dense'}.


\item {} 
\textbf{\texttt{activations}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- Contains the activations of a net. Same structure as \code{spikerates}.

\item {} 
\textbf{\texttt{spiketrains}} (\emph{\texttt{list, optional}}) -- 
Each entry in \code{spiketrains} contains a tuple
\code{(spiketimes, label)} for each layer of the network (for the first
batch only, and excluding \code{Flatten} layers).

\code{spiketimes} is an array where the last index contains the spike
times of the specific neuron, and the first indices run over the
number of neurons in the layer: (n\_chnls, n\_rows, n\_cols, duration)

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'03Dense'}.


\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_param\_sweep() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_param_sweep}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_param\_sweep}}{\emph{results}, \emph{n}, \emph{params}, \emph{param\_name}, \emph{param\_logscale}}{}
Plot accuracy versus parameter.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{results}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- The accuracy or loss for a number of experiments, each of which used
different parameters.

\item {} 
\textbf{\texttt{n}} (\href{https://docs.python.org/library/functions.html\#int}{\emph{\texttt{int}}}) -- The number of test samples used for each experiment.

\item {} 
\textbf{\texttt{params}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- The parameter values that changed during each experiment.

\item {} 
\textbf{\texttt{param\_name}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- The name of the parameter that varied.

\item {} 
\textbf{\texttt{param\_logscale}} (\emph{\texttt{boolean}}) -- Whether to plot the parameter axis in log-scale.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_pearson\_coefficients() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_pearson_coefficients}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_pearson\_coefficients}}{\emph{spikerates\_batch}, \emph{activations\_batch}, \emph{path=None}}{}
Plot the Pearson correlation coefficients for each layer, averaged over one
mini batch.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{spikerates\_batch}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- 
Each entry in \code{spikerates\_batch} contains a tuple
\code{(spikerates, label)} for each layer of the network (for the first
batch only, and excluding \code{Flatten} layers).

\code{spikerates} contains the average firing rates of all neurons in a
layer. It has the same shape as the original layer, e.g.
(batch\_size, n\_features, n\_rows, n\_cols) for a convolution layer.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'03Dense'}.


\item {} 
\textbf{\texttt{activations\_batch}} (\href{https://docs.python.org/library/functions.html\#list}{\emph{\texttt{list}}}) -- Contains the activations of a net. Same structure as
\code{spikerates\_batch}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_potential() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_potential}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_potential}}{\emph{times}, \emph{layer}, \emph{showLegend=False}, \emph{path=None}}{}
Plot the membrane potential of a layer.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{times}} (\emph{\texttt{1D array}}) -- The time values where the potential was sampled.

\item {} 
\textbf{\texttt{layer}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- 
\code{(vmem, label)}.

\code{vmem} is a 2D array where the first index runs over the number of
neurons in the layer, and the second index contains the membrane
potential of the specific neuron.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'3Dense'}.


\item {} 
\textbf{\texttt{showLegend}} (\emph{\texttt{boolean, optional}}) -- If \code{True}, shows the legend indicating the neuron indices and lines
like \code{v\_thresh}, \code{v\_rest}, \code{v\_reset}. Recommended only for layers
with few neurons.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_rates\_minus\_activations() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_rates_minus_activations}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_rates\_minus\_activations}}{\emph{rates}, \emph{activations}, \emph{label}, \emph{path=None}}{}
Plot spikerates minus activations for a specific layer.

Spikerates and activations are each normalized before subtraction.
The neurons in the layer are arranged in a line or on a 2D grid, depending
on layer type.

Activity is encoded by color.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{rates}} (\href{https://docs.python.org/library/array.html\#module-array}{\emph{\texttt{array}}}) -- The spikerates of a layer. The shape is that of the original layer,
e.g. (32, 28, 28) for 32 feature maps of size 28x28.

\item {} 
\textbf{\texttt{activations}} (\href{https://docs.python.org/library/array.html\#module-array}{\emph{\texttt{array}}}) -- The activations of a layer. The shape is that of the original layer,
e.g. (32, 28, 28) for 32 feature maps of size 28x28.

\item {} 
\textbf{\texttt{label}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Layer label.

\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_spiketrains() (in module snntoolbox.io\_utils.plotting)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.plotting.plot_spiketrains}\pysiglinewithargsret{\code{snntoolbox.io\_utils.plotting.}\bfcode{plot\_spiketrains}}{\emph{layer}, \emph{path=None}}{}
Plot which neuron fired at what time during the simulation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{layer}} (\href{https://docs.python.org/library/functions.html\#tuple}{\emph{\texttt{tuple}}}) -- 
\code{(spiketimes, label)}.

\code{spiketimes} is a 2D array where the first index runs over the number
of neurons in the layer, and the second index contains the spike times
of the specific neuron.

\code{label} is a string specifying both the layer type and the index,
e.g. \code{'3Dense'}.


\item {} 
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- If not \code{None}, specifies where to save the resulting image. Else,
display plots without saving.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{common}
\label{snntoolbox.io_utils:common}\label{snntoolbox.io_utils:module-snntoolbox.io_utils.common}\index{snntoolbox.io\_utils.common (module)}
Functions to load various properties of interest in analog and spiking neural
networks from disk.

Created on Wed Nov 18 13:38:46 2015

@author: rbodo
\index{confirm\_overwrite() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.confirm_overwrite}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{confirm\_overwrite}}{\emph{filepath}}{}
If settings{[}'overwrite'{]}==False and the file exists, ask user if it should
be overwritten.

\end{fulllineitems}

\index{download\_dataset() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.download_dataset}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{download\_dataset}}{\emph{fname}, \emph{origin}, \emph{untar=False}}{}
Download a dataset, if not already there.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{fname}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Full filename of dataset, e.g. \code{mnist.pkl.gz}.

\item {} 
\textbf{\texttt{origin}} (\href{https://docs.python.org/library/string.html\#module-string}{\emph{\texttt{string}}}) -- Location of dataset, e.g. url
\url{https://s3.amazonaws.com/img-datasets/mnist.pkl.gz}

\item {} 
\textbf{\texttt{untar}} (\emph{\texttt{boolean, optional}}) -- If \code{True}, untar file.

\end{itemize}

\item[{Returns}] \leavevmode
\textbf{fpath} -- The path to the downloaded dataset. If the user has write access to
\code{home}, the dataset will be stored in \code{\textasciitilde{}/.snntoolbox/datasets/},
otherwise in \code{/tmp/.snntoolbox/datasets/}.

\item[{Return type}] \leavevmode
\href{https://docs.python.org/library/string.html\#module-string}{string}

\end{description}\end{quote}

\begin{notice}{note}{Todo}

Test under python2.
\end{notice}

\end{fulllineitems}

\index{load\_dataset() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.load_dataset}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{load\_dataset}}{\emph{path}, \emph{filename}}{}
Load dataset from an \code{.npy} or \code{.npz} file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{path}} (\emph{\texttt{string, optional}}) -- Location of dataset to load.

\item[{Returns}] \leavevmode
\begin{itemize}
\item {} 
\emph{The dataset as a numpy array containing samples. Example}

\item {} 
With original data of the form (channels, num\_rows, num\_cols), \code{X\_train}

\item {} 
and \code{X\_test} have dimension (num\_samples, channels*num\_rows*num\_cols) for

\item {} 
\emph{a fully-connected network, and (num\_samples, channels, num\_rows, num\_cols)}

\item {} 
\emph{otherwise.}

\item {} 
\code{Y\_train} and \code{Y\_test} have dimension (num\_samples, num\_classes).

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{load\_parameters() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.load_parameters}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{load\_parameters}}{\emph{filepath}}{}
Load all layer parameters from an HDF5 file.

\end{fulllineitems}

\index{to\_categorical() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.to_categorical}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{to\_categorical}}{\emph{y}, \emph{nb\_classes}}{}
Convert class vector to binary class matrix.

If the input \code{y} has shape (\code{nb\_samples},) and contains integers from 0
to \code{nb\_classes}, the output array will be of dimension
(\code{nb\_samples}, \code{nb\_classes}).

\end{fulllineitems}

\index{to\_json() (in module snntoolbox.io\_utils.common)}

\begin{fulllineitems}
\phantomsection\label{snntoolbox.io_utils:snntoolbox.io_utils.common.to_json}\pysiglinewithargsret{\code{snntoolbox.io\_utils.common.}\bfcode{to\_json}}{\emph{data}, \emph{path}}{}
Write \code{data} dictionary to \code{path}.

A \code{TypeError} is raised if objects in \code{data} are not JSON serializable.

\end{fulllineitems}



\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{c}
\item {\texttt{core.inisim}}, \pageref{core:module-core.inisim}
\item {\texttt{core.megasim}}, \pageref{core:module-core.megasim}
\item {\texttt{core.pipeline}}, \pageref{core:module-core.pipeline}
\item {\texttt{core.util}}, \pageref{core:module-core.util}
\indexspace
\bigletter{s}
\item {\texttt{snntoolbox.config}}, \pageref{configure_toolbox:module-snntoolbox.config}
\item {\texttt{snntoolbox.io\_utils.caltech101\_load}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.caltech101_load}
\item {\texttt{snntoolbox.io\_utils.cifar10\_load}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.cifar10_load}
\item {\texttt{snntoolbox.io\_utils.common}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.common}
\item {\texttt{snntoolbox.io\_utils.facedetection\_load}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.facedetection_load}
\item {\texttt{snntoolbox.io\_utils.mnist\_load}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.mnist_load}
\item {\texttt{snntoolbox.io\_utils.plotting}}, \pageref{snntoolbox.io_utils:module-snntoolbox.io_utils.plotting}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
