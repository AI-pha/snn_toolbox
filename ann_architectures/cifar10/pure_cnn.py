# -*- coding: utf-8 -*-
"""

see http://arxiv.org/pdf/1412.6806.pdf

Should get to 7.25% error when using data-augmentation (9.08% without).

Created on Fri Aug 19 09:15:25 2016

@author: rbodo
"""


from __future__ import absolute_import
from __future__ import print_function

from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers.core import Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, AveragePooling2D
from keras.utils import np_utils
# from keras.regularizers import l2
from keras.optimizers import Adam  # SGD

from snntoolbox.io_utils.plotting import plot_history

# Train for 350 epochs and reduce learning rate by factor 0.1 at epochs
# [200, 250, 300]
lr = 0.001
nb_epoch = 1
batch_size = 128
nb_classes = 10

# Input image dimensions
img_rows, img_cols = 32, 32
img_channels = 3

init = 'he_uniform'
reg = None  # l2(0.001)
border_mode = 'same'

# Data set
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

model = Sequential()

model.add(Dropout(0.2, input_shape=(img_channels, img_rows, img_cols)))

model.add(Convolution2D(96, 3, 3, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(96, 3, 3, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(96, 3, 3, init=init, subsample=(2, 2),
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Convolution2D(192, 3, 3, border_mode=border_mode, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(192, 3, 3, border_mode=border_mode, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(192, 3, 3, init=init, subsample=(2, 2),
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Convolution2D(192, 3, 3, border_mode=border_mode, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(192, 1, 1, border_mode=border_mode, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(Convolution2D(10, 1, 1, border_mode=border_mode, init=init,
                        W_regularizer=reg, b_regularizer=reg))
model.add(Activation('relu'))
model.add(AveragePooling2D(pool_size=(6, 6), strides=(1, 1)))
model.add(Flatten())
model.add(Activation('softmax'))

optimizer = Adam(lr)  # SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True)
model.compile(optimizer, 'categorical_crossentropy', metrics=['accuracy'])

# Whether to apply global contrast normalization and ZCA whitening
gcn = True
zca = True

traingen = ImageDataGenerator(rescale=1./255, featurewise_center=gcn,
                              featurewise_std_normalization=gcn,
                              zca_whitening=zca, horizontal_flip=True,
                              rotation_range=10, width_shift_range=0.1,
                              height_shift_range=0.1)

# Compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
traingen.fit(X_train/255.)

trainflow = traingen.flow(X_train, Y_train, batch_size=batch_size)

testgen = ImageDataGenerator(rescale=1./255, featurewise_center=gcn,
                             featurewise_std_normalization=gcn,
                             zca_whitening=zca)

testgen.fit(X_test/255.)

testflow = testgen.flow(X_test, Y_test, batch_size=batch_size)

# Fit the model on the batches generated by datagen.flow()
history = model.fit_generator(trainflow, nb_epoch=nb_epoch,
                              samples_per_epoch=X_train.shape[0],
                              validation_data=testflow,
                              nb_val_samples=len(X_test))
plot_history(history)

score = model.evaluate_generator(testflow, val_samples=len(X_test))
print('Test score:', score[0])
print('Test accuracy:', score[1])

model.save('{:2.2f}.h5'.format(score[1]*100))
